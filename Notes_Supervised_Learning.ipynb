{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#D3A550'>Supervised Learning with Scikit-Learning</font>\n",
    "##### Source: Datacamp, w3Schools, geekforgeeks, and ChatGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Machine Learning: \n",
    "\tThe art and science of giving computers the ability to learn to make decisions\n",
    "\teg: Predict email is spam or not\n",
    "\n",
    "\t1) Supervised learning: Uses labeled data\n",
    "\t\t-Classification: target variable consists of categories eg: YES/NO, 1/0, R/G/B\n",
    "\t\t-Regression: Target variable is continuous: House Pricing\n",
    "\t2) Unsupervised learning: Uses unlabeled data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Important naming conventions:\n",
    "\n",
    "Features = predictor variables = independent variables, also known as the feature columns/dimensions\n",
    "\n",
    "Target = dependent variable = response variable, also known as output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Libraries for Machine Learning:\n",
    "Scikit-Learn, TensorFlow, keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Exploratory data analysis or EDA\n",
    "\t- Numerical EDA:\n",
    "        - Calculating basic statistics such as mean, median, mode, standard deviation, and quartiles for each numerical feature\n",
    "        - Calculating the correlation matrix of all numerical features to identify potential correlation among features\n",
    "        - Checking for outliers using box plots, scatter plots, and z-scores\n",
    "\n",
    "    - Visual EDA:\n",
    "        - histograms to visualize the distribution of individual numerical features\n",
    "        - scatter plots to visualize the relationship between two numerical features\n",
    "        - bar plots and pie charts to visualize the distribution of categorical features\n",
    "        - pair plots and heat maps to visualize the relationship between multiple numerical features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) .fit() VS .predict()\n",
    "\n",
    ".fit() \n",
    "\t- train a model on a given datasets\n",
    "\t- takes 2 arguments(X_train, y_train) where X_train is the features, and y_train is the output/target variable\n",
    "\t\n",
    ".predict() \n",
    "\t- make predictions on new data using trained model\n",
    "\t- takes 1 argument, the new data to make prediction eg: .predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) kNN or k-Nearest Neighbors: one of the many supervised learning algorithm\n",
    "\t-Predict the label of a data point by looking at 'k' closest labeled data points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Train and Test\n",
    "-Train/Test Split: in machine learning, data is usually split into 2: train(train a model) and test(evaluate performance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original dataset:\n",
    "\n",
    "| id | X1 | X2 | Y  |\n",
    "|----|----|----|----|\n",
    "| 1  | 3  | 4  | 1  |\n",
    "| 2  | 5  | 2  | 0  |\n",
    "| 3  | 8  | 6  | 1  |\n",
    "| 4  | 1  | 9  | 0  |\n",
    "| 5  | 4  | 2  | 1  |\n",
    "\n",
    "Training set:\n",
    "\n",
    "| id | X1 | X2 | Y  |\n",
    "|----|----|----|----|\n",
    "| 1  | 3  | 4  | 1  |\n",
    "| 3  | 8  | 6  | 1  |\n",
    "| 4  | 1  | 9  | 0  |\n",
    "\n",
    "Test set:\n",
    "\n",
    "| id | X1 | X2 | Y  |\n",
    "|----|----|----|----|\n",
    "| 2  | 5  | 2  | 0  |\n",
    "| 5  | 4  | 2  | 1  |\n",
    "\n",
    "Here, the training set is used to train a model. The test set is used to evaluate the performance of the model.\n",
    "The model will use the input features X1 and X2 of the training set to learn the relationship between the input and output variables \n",
    "it will use the input features of the test set to predict the output variable and compare it with the actual output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) egg kNN using digits datasets in sklearn\n",
    "\n",
    "### Original dataset (digits):\n",
    "\n",
    "| id | X1 | X2 | X3 | ... | X64 | Y  |\n",
    "|----|----|----|----|-----|-----|----|\n",
    "| 1  | 0  | 1  | 5  | ... | 2   | 3  |\n",
    "| 2  | 4  | 0  | 0  | ... | 7   | 2  |\n",
    "| 3  | 3  | 7  | 9  | ... | 1   | 4  |\n",
    "| 4  | 1  | 0  | 2  | ... | 8   | 1  |\n",
    "| 5  | 2  | 8  | 8  | ... | 6   | 9  |\n",
    "\n",
    "Training set:\n",
    "\n",
    "| id | X1 | X2 | X3 | ... | X64 | Y  |\n",
    "|----|----|----|----|-----|-----|----|\n",
    "| 1  | 0  | 1  | 5  | ... | 2   | 3  |\n",
    "| 3  | 3  | 7  | 9  | ... | 1   | 4  |\n",
    "| 4  | 1  | 0  | 2  | ... | 8   | 1  |\n",
    "| 5  | 2  | 8  | 8  | ... | 6   | 9  |\n",
    "\n",
    "Test set:\n",
    "\n",
    "| id | X1 | X2 | X3 | ... | X64 | Y  |\n",
    "|----|----|----|----|-----|-----|----|\n",
    "| 2  | 4  | 0  | 0  | ... | 7   | 2  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random state and Stratify\n",
    "random_state: Say you set random_state=42, the function will always split the data \n",
    "in the same way, if you run the script multiple times, \n",
    "you will have the same training and test sets\n",
    "\n",
    "stratify: Say you have a dataset with 90% of samples belonging to class 1 and 10% of samples \n",
    "belonging to class 2, stratify parameter will ensure that the same proportion (90% and 10%)\n",
    "maintained in both the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# load the digits dataset\n",
    "digits = load_digits()\n",
    "# Create feature and target arrays\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create a k-NN classifier with 7 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Linear Regression\n",
    "\n",
    "Linear regression is a machine learning algorithm that predicts a continuous output value based on input features by finding the best fit line through the data points.\n",
    "\n",
    "|                             | KNN                                      | Linear Regression                    |\n",
    "|-----------------------------|------------------------------------------------|--------------------------------------------------|\n",
    "| Method                     | Non-parametric                              | Parametric (assumes linearity)            |\n",
    "| Prediction Type            | Majority class among k nearest examples | Equation of a straight line (y=mx+b)  |\n",
    "| Type of Label              | Discrete                                      | Continuous                                |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.8219419939586892\n",
      "Root Mean Squared Error: 3.4052481157341434\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('datasets\\gm_2008_region.csv')\n",
    "\n",
    "# One-hot encode the 'Region' column\n",
    "df_encoded = pd.get_dummies(df, columns=['Region'], prefix='Region')\n",
    "\n",
    "# Assign the encoded Dataframe to X\n",
    "X = df_encoded.drop(['life'], axis=1)\n",
    "y = df_encoded['life']\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the regressor\n",
    "reg_all = LinearRegression()\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "reg_all.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = reg_all.predict(X_test)\n",
    "\n",
    "# Compute and print R^2 and RMSE\n",
    "print(\"R^2: {}\".format(r2_score(y_test, y_pred)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Cross validation technique\n",
    "\n",
    "Cross-validation is a technique used to evaluate the performance of a machine learning model by training the model on different subsets of the data and evaluating it on the remaining parts. The most common one is k-fold cross-validation. In k-fold cross-validation, the data is divided into k subsets, or \"folds\", and the model is trained on k-1 of the folds and evaluated on the remaining one. This process is repeated k times, with a different fold being used as the test set each time.\n",
    "\n",
    "cross_val_score() is a function from the sklearn.model_selection module that allows you to easily perform k-fold cross-validation on a model. \n",
    "The function takes the following arguments:\n",
    "    estimator: the model you want to evaluate eg: LinearRegression()\n",
    "    X: the feature data\n",
    "    y: the target data\n",
    "    cv: the number of folds to use (k)\n",
    "\n",
    "Note that cross_val_score will take care of splitting the data into training and test sets for each fold and fitting the model to the training data and evaluating it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858814795800147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('datasets\\gm_2008_region.csv')\n",
    "\n",
    "# One-hot encode the 'Region' column\n",
    "df_encoded = pd.get_dummies(df, columns=['Region'], prefix='Region')\n",
    "\n",
    "# Assign the encoded Dataframe to X\n",
    "X = df_encoded.drop(['life'], axis=1)\n",
    "y = df_encoded['life']\n",
    "\n",
    "# Create the regressor\n",
    "reg_all = LinearRegression()\n",
    "\n",
    "# Perform 3-fold CV\n",
    "cvscores_3 = cross_val_score(reg_all, X, y, cv=3)\n",
    "print(np.mean(cvscores_3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is the process of searching for the best combination of hyperparameter for a machine learning model to achieve the best performance on unseen data.\n",
    "Examples of hyperparameter include  the number of nearest neighbors (N) in KNN and the regularization strength in logistic regression (C)\n",
    "\n",
    "GridSearchCV: Popular method for hyperparameter tuning in scikit-learn. It's used to search for the best combination of hyperparameters by training the model on different combinations of the hyperparameters and evaluating its performance using cross-validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.006105402296585327}\n",
      "Best score is 0.7734742381801205\n"
     ]
    }
   ],
   "source": [
    "# Example of hyperparameter tuning with logistic regression using diabetes datasets with GridSearchCV\n",
    "# Parameters for LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df = pd.read_csv('datasets\\diabetes.csv')\n",
    "\n",
    "# Assign the encoded Dataframe to X\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 4, 'min_samples_leaf': 1}\n",
      "Best score is 0.7448094389270861\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with RandomizedSearchCV with diabetes dataset\n",
    "\n",
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Machine Learning Pipeline\n",
    "\n",
    "<font color='#C0B3A0'>As we've seen before all this model follow a similar procedure or steps to predict the model below is the general overview of how a basic machine learning pipeline will work\n",
    "1) Data Loading: Loading datasets either CSV, JSON, txt etc\n",
    "\n",
    "2) Data Cleaning and Preprocessing: You can clean and preprocess the data to make it ready for analysis. This can include tasks such as handling missing values, removing outliers, converting data types, normalizing data, and more.\n",
    "\n",
    "3) Exploratory Data Analysis (EDA): Start by performing EDA on the data, and looking for patterns and relationships between the different variables. Create visualizations, such as histograms, box plots, scatter plots, and heat maps, to better understand the distribution of the data and identify any outliers. This will give you a better understanding of the data and help you identify any potential problems or issues.\n",
    "\n",
    "4) Data visualization: Create data visualizations to represent patterns and relationships in the data. Use different types of charts to represent different aspects of the data, such as bar charts for categorical data and scatter plots for continuous data. You can also create interactive visualizations using libraries like plotly, bokeh, etc.\n",
    "\n",
    "5) Feature Selection: Use feature selection techniques, such as linear discriminant analysis (LDA) and principal component analysis (PCA), to identify which variables are most important in making prediction.\n",
    "\n",
    "6) Correlation Analysis: Perform correlation analysis to find out the correlation between different variables and the target columns. This will help you identify which variables are the most important predictors of mental wellbeing and life satisfaction.\n",
    "\n",
    "7) Clustering: This step falls under the category of !!Unsupervised learning!! and Data Exploration. It's used to group similar data points together by creating a tree-like structure called a dendrogram.\n",
    "\n",
    "8) Predictive Modeling: Use the data to train machine learning models to predict one of the columns. You can try different models, such as linear regression, decision trees, random forests, and neural networks, and compare their performance.\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 22:25:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ea266b9a32e539376479ecefb4b43de94cddfc186952a2290bc119c664ac37e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
